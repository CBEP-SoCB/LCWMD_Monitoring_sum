---
title: "Summary of Analysis of LCWMD Dissolved Oxygen Data"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "01/06/2021"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 4
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
The Long Creek Watershed, almost three and a half square miles in area, is
dominated by commercial land use. The Maine Mall is one of the largest land
owners in the watershed, and it is surrounded by a range of commercial
businesses, from medical offices, to car washes.  About a third of the watershed
in impervious surfaces like roads, parking lots, and rooftops.

Landowners with an acre or more of impervious area are required to get a Clean
Water Act permit for stormwater discharges from their property.  The LCWMD
provides an alternative for landowners to working to receive an individual
permit. Landowners who elect to participate in the The Long Creek Watershed
Management District receive a General Permit, in return for providing funding to
the District, and facilitating the work of the district by permitting access to
their property for certain activities.

For more information on LCWMD, see [their web site](restorelongcreek.org).

Over the past decade, LCWMD has contracted with several consulting firms to
provide  water quality monitoring services along Long Creek.  This has produced
one of the most extensive and best documented data set from the Northeastern US 
looking at water quality conditions in an urban stream.

GZA Geoenvironmental Incorporated (GZA) has been the primary monitoring
contractor for LCWMD in recent years, and in 2019, they conducted a thorough
review of LCWMD data. These analyses are based on their summary data sets, and
recapitulate and extend their analyses.

## Are Water Quality Criteria Met?
The primary question we ask in this Notebook, is whether water quality criteria 
pertaining to levels of dissolved oxygen are met. In poarticular, we explore
various ways of modelling those probabilities, and settle on modelling only 
summertime probabilities as the most informative for State of Casco Bay readers.

We ask whether the probability of failing to meet criteria each day is
changing.  Secondarily, we examine differences among sites in the probability of
failing criteria.

In this data set a "TRUE" value consistently implies that water quality criteria
were met or exceeded, whether that is achieved by a value higher than or lower
than some numeric criteria.  "TRUE" implies good conditions.  "FALSE" implies 
bad conditions.
    
## Sources of Threshold Values  
### Dissolved oxygen
Maine’s Class B water quality standards call for dissolved oxygen above 7 mg/l,
with percent saturation above 75%. The Class C Standards, which apply to almost
all of Long Creek, call for dissolved oxygen above 5 mg/l, with percent
saturation above 60%. In addition, for class C conditions, the thirty day
average dissolved oxygen muststay above 6.5 mg/l.

### Chloride
Maine uses established thresholds for both chronic and acute exposure to
chloride. These are the “CCC and CMC” standards for chloride in freshwater.
(06-096 CMR 584). These terms are defined in a footnote as follows:

>   The Criteria Maximum Concentration (CMC) is an estimate of the highest
    concentration of a material in surface water to which an aquatic community
    can be exposed briefly without resulting in an unacceptable effect. The
    Criterion Continuous Concentration (CCC) is an estimate of the highest
    concentration of a material in surface water to which an aquatic community
    can be exposed indefinitely without resulting in an unacceptable effect.

The relevant thresholds are:

*   Chloride CCC  = 230  mg/l
*   Chloride CMC  = 860  mg/l

In practice, chloride in Long Creek are indirectly estimated based on 
measurement of conductivity.  The chloride-conductivity correlations is fairly
close and robust, but estimation is an additional source of error, although 
generally on the level of 10% or less.

### Temperature
There are no legally binding Maine criteria for maximum stream temperature, but
we can back into thresholds based on research on thermal tolerance of brook
trout in streams. A study from Michigan and Wisconsin, showed that trout are
found in streams with daily mean water temperatures as high as 25.3°C, but only
if the period of exceedence of that daily average temperature is short – only
one day. Similarly, the one day daily maximum temperature above which trout were
never found was 27.6°C. That generates two temperature criteria, one for daily
averages, and one for daily maximums. 

These criteria should be taken as rough values only, as the  original study was
observational, and thus the key driver of suitability for trout could be
another stressor correlated with these temperature metrics.

>  Wehrly, Kevin E.; Wang, Lizhu; Mitro, Matthew (2007). “Field‐Based Estimates
   of Thermal Tolerance Limits for Trout: Incorporating Exposure Time and
   Temperature Fluctuation.” Transactions of the American Fisheries Society
   136(2):365-374.

# Import Libraries  
```{r}
library(tidyverse)
library(readr)

library(emmeans) # Provides tools for calculating marginal means
library(nlme)

#library(zoo)     # here, for the `rollapply()` function

library(mgcv)    # generalized additive models. Function gamm() allows
                 # autocorrelation.

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

# Data Preparation
## Initial Folder References
```{r}
sibfldnm    <- 'Original_Data'
parent      <- dirname(getwd())
sibling     <- file.path(parent,sibfldnm)

dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
dir.create(file.path(getwd(), 'models'), showWarnings = FALSE)
```

## Load Weather Data
```{r load_weather_data}
fn <- "Portland_Jetport_2009-2019.csv"
fpath <- file.path(sibling, fn)

weather_data <- read_csv(fpath, 
 col_types = cols(.default = col_skip(),
        date = col_date(),
        PRCP = col_number(), PRCPattr = col_character() #,
        #SNOW = col_number(), SNOWattr = col_character(), 
        #TMIN = col_number(), TMINattr = col_character(), 
        #TAVG = col_number(), TAVGattr = col_character(), 
        #TMAX = col_number(), TMAXattr = col_character(), 
        )) %>%
  rename(sdate = date) %>%
  mutate(pPRCP = dplyr::lag(PRCP))
```

## Update Folder References
```{r}
sibfldnm    <- 'Derived_Data'
parent      <- dirname(getwd())
sibling     <- file.path(parent,sibfldnm)
```

## Load Data on Sites and Impervious Cover
These data were derived from Table 2 from a GZA report to the Long Creek
Watershed Management District, titled "Re: Long Creek Watershed Data Analysis;
Task 2: Preparation of Explanatory and Other Variables."  The Memo is dated
November 13, 2019 File No. 09.0025977.02.

Cumulative Area and IC calculations are our own, based on the GZA data and the
geometry of the stream channel.

```{r}
# Read in data and drop the East Branch, where we have no data
fn <- "Site_IC_Data.csv"
fpath <- file.path(sibling, fn)

Site_IC_Data <- read_csv(fpath) %>%
  filter(Site != "--") 

# Now, create a factor that preserves the order of rows (roughly upstream to downstream). 
Site_IC_Data <- Site_IC_Data %>%
  mutate(Site = factor(Site, levels = Site_IC_Data$Site))

# Finally, convert percent covers to numeric values
Site_IC_Data <- Site_IC_Data %>%
  mutate(CumPctIC = as.numeric(substr(CumPctIC, 1, nchar(CumPctIC)-1))) %>%
  mutate(PctIC = as.numeric(substr(PctIC, 1, nchar(PctIC)-1)))
Site_IC_Data
```

## Load Main Data
Read in the data from the Derived Data folder.

Note that I filter out data from 2019 because that is only a partial year, which might affect estimation of things like seasonal trends.  We could add it back in, but with care....

*Full_Data.csv* does not include a field for precipitation from the
previous day.  In earlier work, we learned that a weighted sum of recent
precipitation provided better explanatory power.  But we also want to check
a simpler model, so we construct a "PPrecip" data field.  This is based
on a modification of code in the "Make_Daily_Summaries.Rmd" notebook.

```{r}
fn <- "Full_Data.csv"
fpath <- file.path(sibling, fn)

full_data <- read_csv(fpath, 
    col_types = cols(DOY = col_integer(), 
        D_Median = col_double(), Precip = col_number(), 
        X1 = col_skip(), Year = col_integer())) %>%

  mutate(Site = factor(Site, levels=levels(Site_IC_Data$Site)),
         Month = factor(Month, levels = month.abb),
         Year_f = factor(Year),
         IC=as.numeric(Site_IC_Data$CumPctIC[match(Site, Site_IC_Data$Site)])) %>%

# We combine data using "match" because we have data for multiple sites and 
# therefore dates are not unique.  `match()` correctly assigns weather
# data by date.
  mutate(PPrecip = weather_data$pPRCP[match(sdate, weather_data$sdate)])

```

### Cleanup
```{r}
rm(Site_IC_Data, weather_data)
rm(fn, fpath, parent, sibling, sibfldnm)
```

## Data Corrections
### Anomolous Depth Values
Several depth observations in the record appear highly unlikely. In particular,
several observations show daily median water depths over 15 meters. And
those observations were recorded in May or June, at site S05, with no associated 
record of significant precipitation, and no elevated depths at other sites on 
the stream.

We can trace these observations back to the raw QA/QC'd pressure and sonde data 
submitted to LCWMD by GZA, so they are not an artifact of our data preparation.

A few more observations show daily median depths over 4 meters, which also
looks unlikely in a stream of this size.  All these events also occurred in 
May or June of 2015 at site S05. Some sort of malfunction of the pressure 
transducer appears likely.

We remove these extreme values.  The other daily medians in May and June of 2015
appear reasonable, and we leave them in place, although given possible 
instability of the pressure sensors, it might make sense to remove them all.
```{r}
full_data <- full_data %>%
  mutate(D_Median = if_else(D_Median > 4, NA_real_, D_Median),
         lD_Median = if_else(D_Median > 4, NA_real_, lD_Median))
```

### Single S06B Chloride Observation from 2017
The data includes just a single chloride observation from site S06B from
any year other than 2013.  While we do not know if the data point is legitimate
or not, it has very high leverage in several models, and we suspect a 
transcription error of some sort.
```{r fig.width = 3, fig.height = 2}
full_data %>%
  filter(Site == 'S06B') %>%
  select(sdate, DO_Median) %>%
  ggplot(aes(x = sdate, y = DO_Median)) + geom_point()
```

We remove the Chloride value from the data.
```{r}
full_data <- full_data %>%
  mutate(Chl_Median = if_else(Site == 'S06B' & Year > 2014,
                              NA_real_, Chl_Median))
```

### Anomolous Dissolved Oxygen and Chloride Values
#### Site S03, end of 2016
We noted some extreme dissolved oxygen data at the end of 2016.  Values were
both extreme and highly variable.
```{r}
full_data %>% 
  filter (Year == 2016, Site == 'S03', Month %in% c("Oct", "Nov", "Dec")) %>%
ggplot(aes(x = sdate)) + 
  geom_point(aes(y = DO_Median)) +
  geom_line(aes(y = DO_Median)) +
  geom_line(aes(y = D_Median * 20), color = 'blue', lty = 2) +
  geom_line(aes(y = Chl_Median / 20), color = 'green', lty = 2) +
  geom_line(aes(y = MaxT), color = 'red', lty = 2) +
  theme_cbep(base_size = 10) +
  theme(legend.position="bottom", legend.box = "vertical") +
  
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 4, 
           label = 'Max Air Temp', color = 'red') +
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 2, 
           label = 'Depth x 20', color = 'blue') +
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 0, 
           label = 'Chlorides / 20', color = 'green')
```

The unstable behavior after October 23rd is questionable.  DO does not normally fluctuate so widely. Percent saturation associated with these observations extend as high as 200%.  There is clearly a problem.

We looked at the raw data, and examined the time course of all sonde-related data.

*  There was a sudden decline in observed DO at about 1:00 am on October 23rd,
   2016. That corresponds to a simultaneous rise in conductivity / chlorides,
   and follows a brief rise and rapid fall in water temperature.  
*  Recovery in DO several days later (1/28/2016) corresponds to a drop in
   chlorides, and a BREIF increase on water depth, but there is no related
   change in temperature.  
*  Ongoing brief spikes in DO appear to correspond to drops in chlorides
   or conductivity, and very brief small blips in water depth.  
*  The data record includes stable data at lower water depths, so it is unlikely
   that the sensors were exposed.  On the other hand, they could have been
   buried by sediment.  Air temperatures were not low enough to suggest that the
   sensors may have frozen, or battery power may have been failing.
   
The raw data makes it clear that whatever was going on affected both
conductivity and dissolved oxygen measurements, but did not obviously affect
temperature or pressure (water depth).  There are two possible interpretations.
Either the data are real, and those exceptionally high DO and percent saturation
values are real, or there was some sort of malfunction that affected both
chloride and dissolved oxygen.

We decide we should remove chloride and oxygen observations after October 15th.
Site S03 shows some low dissolved oxygen, high chloride observations from
November of 2015 as well, but the raw data is smoother, without the extreme high 
values, and without zero DO observations.  We leave those data in place as 
likely correct.

```{r}
full_data <- full_data %>% 
  mutate(Chl_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, Chl_Median),
         DO_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, DO_Median),
         PctSat_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, PctSat_Median))
```


## Remove Partial Data from Winter Months
We have very limited data from several months.  We have January data 
from only one year, and February data from only three, and December data from 
four, all older.  Both March and November sample sizes vary.

The limited winter data generates severely unbalanced samples, which may lead to
estimation problems, especially in models with crossed or potentially crossed
factors and predictors.  More fundamentally, the potential bias introduced by
showing data from those months from just a handful of years could give a
misleading impression of seasonal patterns.  We trim December, January and
February data, but leave the other months.

It is important to remember,  even after trimming the data, that:  
1.  2010 is a partial year,  
2.  The period of sampling in March may be biased due to spring melt timing.

```{r month_by_year_crosstab}
xtabs(~ Year_f + Month, data = full_data)
```

```{r remove_limited_winter_data}
full_data <- full_data %>%
  filter(Month %in% month.abb[3:11]  )
```

## Add Stream Flow Index
We worked through many models on a site by site basis in which we included data
on water depth, but since the depth coordinate is site-specific, a 10 cm depth
at one site may be exceptional, while at another it is commonplace. We generally
want not a local measure of stream depth, but a watershed-wide metric of high,
medium, or low stream flow.

Middle and Lower Maine Stem sites would be suitable for a general flow indicator
across the watershed. The monitoring sites in that stretch of Long Creek include
include S05 and S17, however only site S05 has been in continuous operation
throughout the period of record, so we use depth data from S05 to construct
our general stream flow indicator.

Stream flow at S05 is correlated with flow at other sites, although not all that
closely correlated to flow in the downstream tributaries.

```{r}
full_data %>%
  select(sdate, Site, lD_Median) %>%
  pivot_wider(names_from = Site, values_from = lD_Median) %>%
  select(-sdate) %>%
  cor(use = 'pairwise', method = 'pearson')
```

We use the log of the daily median flow at S05 as a general watershed-wide
stream flow indicator, which we call `FlowIndex`.  We use the log of the raw
median, to lessen the effect of the highly skewed distribution of stream depths
on the metric.

```{r}
depth_data <- full_data %>%
  filter (Site == 'S05') %>%
  select(sdate, lD_Median)

full_data <- full_data %>%
  mutate(FlowIndex = depth_data$lD_Median[match(sdate, depth_data$sdate)])
  rm(depth_data)
```

Note that because the flow record at S05 has some gaps, any model using this
predictor is likely to have a smaller sample size.

## Select Final Data Set
```{r}
full_data <- full_data %>%
  mutate(Year_f = factor(Year)) %>%
  select (Site, sdate, Year, Year_f, Month, DOY, 
          Precip, lPrecip, PPrecip, wlPrecip, MaxT,
          D_Median, lD_Median,
          DO_Median, PctSat_Median, T_Median, Chl_Median,
          IC, FlowIndex) %>%
  filter(! is.na(DO_Median))
```

# Crosstabs
```{r}
xtabs(~ Site + Year, data = full_data)
```

Note that Site S05 and S17 have partially complementary data histories on
dissolved oxygen.

```{r}
xtabs(~ Month + Year + Site, data = full_data)
```

# Exploratory Graphics
```{r}
ggplot(full_data, aes(x = DO_Median)) + geom_histogram(aes(fill = Site))
```
So, while not exactly normally distributed, it's not wildly skewed either.

```{r fig.height = 7, fig.width = 7}
ggplot(full_data, aes(y = DO_Median, x = T_Median)) + 
  geom_point(aes(shape = Site, color = Month)) +
  geom_smooth() +
  theme_cbep(base_size = 10) +
  theme(legend.position="bottom", legend.box = "vertical")
```
S01 and S05 have high temperature low or no DO events. S03 has some mid
temperature low DO events, and SO7 and S05 have low temperature low DO events.
Those low DO events tend to occur in temporal clusters, all part of one year and
one month.

Low DO conditions occur infrequently, but when the do occur, they tend to occur
in clusters.

```{r}
ggplot(full_data, aes(y = DO_Median, x = sdate, color = Site)) + 
  geom_point() +
  geom_smooth(method = 'gam', formula = y ~ s(x, bs = "cs", k = 9*3)) + 
  theme_cbep(base_size = 10) 
```


#Linear Model With Autocorrelated Errors
Recall that GLS does not handle incomplete designs.  Since we don't have data
from all combinations of sites and years, we can not include a sites by years 
interaction in the model.  From other experience, we expect such a term would be 
"significant", but possibly not important.

This takes a long time to complete -- about 20 minutes. It's big messy cross 
correlation model, even without including any covariates.
```{r}
if (! file.exists("models/do_gls.rds")) {
  print(
    system.time(
      do_gls <- gls(DO_Median ~ Site + 
                      #lPrecip + 
                      #wlPrecip +
                      #FlowIndex +
                      MaxT +
                      Month +
                      Year_f, #+
                    #Site : Year_f,
                    correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                    na.action = na.omit, 
                    #method = 'REML',
                    data = full_data)
    )
  )
  saveRDS(do_gls, file="models/do_gls_.rds")
} else {
  do_gls <- readRDS("models/do_gls.rds")
}
anova(do_gls)
```

```{r}
summary(do_gls)
```

Results are interesting.  As suspected, dissolved oxygen on successive days are 
highly autocorrelated -- over 95%.  Once you take into account air temperature
and time of year, neither site nor year ends up as statistically significant.
Note that this conflicts with results of the analysis of exceedences, where both
site and year mattered.


## We Explore a Larger Model
```{r}
if (! file.exists("models/do_gls_with.rds")) {
  print(
    system.time(
      do_gls_with <- gls(DO_Median ~ Site +
                      FlowIndex +
                      T_Median +
                      MaxT +
                      Month +
                      Year_f, #+
                    #Site : Year_f,
                    correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                    na.action = na.omit, 
                    #method = 'REML',
                    data = full_data)
    )
  )
  saveRDS(do_gls_with, file="models/do_gls_with.rds")
} else {
  do_gls_with <- readRDS("models/do_gls_with.rds")
}
anova(do_gls_with)
```
Note that in the context of a flow and stream water temperature predictors, we
see a difference between sites. Air temperature is a lot less important  after 
we fit a stream temperature term.  Stream temperature ends up as by far the most 
important predictor. One challenge we have seen in too many other analyses is
that with the large data sets we have, lots of small signals are statistically
significant.

# GAMM Analysis
Here we use "General Additive Models" that allow non-linear (smoother) fits for
some parameters. Our emphasis is on using smoothers to account for
non-linearities in relationships between weather or flow-related predictors and
dissolved oxygen.

We use the function `gamm()` because it has a relatively simple interface for
incorporating autocorrelated errors.

We abuse the autocorrelation model slightly, since we don't fit
separate autocorrelations for each site and season.  That should have little
impact on results, as missing values at beginning and end of most time series
prevent estimation anyway.

## Initial GAM Model
Our first GAM simply fits smoothers for each of the major weather-related
covariates.  Arguably, we should fit separate smoothers by `FlowIndex` for
each site.

This model takes a long time to run minutes to run (more than 5, less than 15)

, form = ~ as.numeric(sdate) | Site

```{r first_gam, cache = TRUE}
system.time(
  do_gam <- gam(DO_Median ~ Site + 
                  s(FlowIndex) +
                  s(MaxT) +
                  s(T_Median) +
                  Month +
                  Year_f +
                  Site : Year_f,
                correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                na.action = na.omit, 
                #method = 'REML',
                data = full_data)
)

anova(do_gam)
```

```{r}
summary(do_gam)
```

The site by year interaction is hard to understand.  We will need to look at
marginal means, but that will be more useful once we handle temporal
autocorrelation.

```{r}
plot(do_gam)
```

The interesting feature here is that effects of both air temperature and water 
temperature are nearly linear, with water temperature much larger.

Lets try shifting the water temp to a linear term, and dropping the air temp
term. We have to be back again in the world where confounded factors will
stop the analysis.  We have to drop the Site by Year term.

## Initial GAMM model
This model is also likely to take approximately 15 minutes to run.
```{r first_gamm, cache = TRUE}
if (! file.exists("models/do_gamm.rds")) {
  print(
    system.time(
      do_gamm <- gamm(DO_Median ~ Site + 
                        T_Median +
                        s(FlowIndex) +
                        Month +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm, file="models/do_gamm.rds")
} else {
  do_gamm <- readRDS("models/do_gamm.rds")
}
```

### ANOVA
```{r}
anova(do_gamm$gam)
```

### Summary
```{r}
summary(do_gamm$gam)
```

### Estimated Daily Autocorrelation
```{r}
summary(do_gamm$lme)$modelStruct$corStruct
```



### Structure of the Smoother
```{r}
plot(do_gamm$gam)
```

### Diagnostic Plots
The help files for `gam.check()` suggest using care when interpreting results
for GAMM models, since the function does not correctly incorporate the error
correlations structure.  However, for our purposes, this is probably sufficient,
since our focus is not on statistical significance, but on estimation.
```{r}
gam.check(do_gamm$gam)
```
What that shows is, unfortunately, that the extreme low DO events -- which are
our highest priority in many ways -- are rather poorly modeled.  And it is clear
the assumptions of normality are not met, especially for those low values.

For careful work, we should probably use bootstrapped confidence intervals or
something similar, but given how long these models take to fit, that is not 
practical.  Besides, it is probably overkill.

### Estimated Marginal Means
Reliably calling `emmeans()` for `gamm()` models requires 
creating a call object and associating it with the model (e.g., as
`do_gamm$gam$call`). (See the `emmeans` "models" vignette for more info, although
not all strategies recommended there worked for us).

We first create the call object, then associate it with the model, and finally
manually construct a reference grid before calling `emmeans()` to extract
marginal means.  This workflow has the advantage that it requires us to think
carefully about the structure of the reference grid.

The default `emmeans()` behavior creates a reference grid where marginal 
means are keyed to mean values of all quantitative predictors, but averaged
across all factors.  Since we fit YEar only asa factor, we do not specify year 
here.

#### By Month
```{r}
the_call <-  quote(gamm(DO_Median ~ Site + 
                        T_Median +
                        s(FlowIndex) +
                        Month +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm$gam$call <- the_call

my_ref_grid <- ref_grid(do_gamm, cov.reduce = median) 
(by_month <- summary(emmeans(my_ref_grid, ~ Month)))
```

```{r}
labl <- 'Values  Flow and\nMedian Daily Water Temperature\nAll Sites Combined'

plot(by_month) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab ('') +
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```


#### By Site
```{r fig.width = 5, fig.height = 4}
(by_site <- summary(emmeans(my_ref_grid, ~ Site)))
```

```{r}
plot(by_site) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab("Upstream         Main Stem          Lower Tribs") +
  annotate('text', 11, 2.5, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)

```

#### By Year
```{r}
my_ref_grid <- ref_grid(do_gamm, cov.reduce = median) 
by_year <- summary(emmeans(my_ref_grid, 'Year_f'))
by_year
```

```{r}
plot(by_year) + 
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab('') +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```
2010 was a partial year, so despite (or perhaps because of) adjusting for
months, the 2010 estimate may be misleading.  Since then, basically, 2016 is 
way worse than the other years.

## Simplified Model
We drop the MONTH term and the FLOW term.  We refit the water temperature term
as a low dimensional smooth, because we need to include at least one smoother in
the GAMM model.
```{r second_gamm, cache = TRUE}
if (! file.exists("models/do_gamm_2.rds")) {
  print(
    system.time(
      do_gamm_2<- gamm(DO_Median ~ Site + 
                        s(T_Median, k = 1) +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_2, file="models/do_gamm_2.rds")
} else {
  do_gamm_2 <- readRDS("models/do_gamm_2.rds")
}
```

### ANOVA
```{r}
anova(do_gamm_2$gam)
```

### Summary
```{r}
summary(do_gamm_2$gam)
```

### Estimated Daily Autocorrelation
```{r}
summary(do_gamm_2$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r}
plot(do_gamm_2$gam)
```


### Diagnostic Plots
```{r}
gam.check(do_gamm_2$gam)
```
The model has essentially the same inadequacies as the prior model.

### Estimated Marginal Means
We again create the call object, and associate it with the model, and finally
manually construct a reference grid before calling `emmeans()` to extract
marginal means.  This workflow has the advantage that it requires us to think
carefully about the structure of the reference grid. We explicitly specify that
we want the marginal means estimated at Year = 2014.

```{r}
the_call <-  quote(gamm(DO_Median ~ Site + 
                        s(T_Median, k = 1) +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_2$gam$call <- the_call
```

#### By Site
```{r fig.width = 5, fig.height = 4}
(by_site_2 <- summary(emmeans(my_ref_grid, ~ Site)))
```

```{r}
plot(by_site_2) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab("Upstream         Main Stem          Lower Tribs") +
  annotate('text', 11, 2.5, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)

```

#### By Year
```{r}
my_ref_grid <- ref_grid(do_gamm_2, cov.reduce = median) 
(by_year_2 <- summary(emmeans(my_ref_grid, 'Year_f')))
```

```{r}
plot(by_year_2) + 
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab('') +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```


# Compare Marginal Means From Two Models
## Calculate Observed Averages
```{r}
avg_by_site <- full_data %>%
  select(Site, DO_Median) %>%
  group_by(Site) %>%
  summarize(observed = mean(DO_Median, na.rm = TRUE),
            .groups = 'drop')  %>%
  pull(observed)

avg_by_year <- full_data %>%
  select(Year_f, DO_Median) %>%
  group_by(Year_f) %>%
  summarize(observed = mean(DO_Median, na.rm = TRUE),
            .groups = 'drop')  %>%
  pull(observed)
```

## By Site
```{r}
tibble(Site = by_site$Site,
       observed = avg_by_site,
       large = by_site$emmean,
       small = by_site_2$emmean ) %>%

ggplot(aes(x = observed)) +
  geom_point(aes(y = small), color = 'red') +
  geom_point(aes(y = large), color = 'blue') +
  geom_text(aes(y = small, label = Site),
            hjust = 0, nudge_x = 0.05, nudge_y = -0.05) +
  geom_abline(slope = 1, intercept = 0) +
  xlab('Observed') +
  ylab('Marginal Means')  +
  xlim(6.5, 9.5) +
  ylim(6.5,9.5)

```

Correspondence with observed means is only so-so, as is expected with uneven
sampling histories. The main difference is in shifting position of S05 and S17.
These sites have inconsistent sampling histories, so marginal means are adjusted 
by year.  S17 was observed principally during "bad" years, so the marginal mean
(which is averaged across for ALL years) as adjusted upwards, since the model
concludes the observed values would probably have been better.  Meanwhile, site 
S05 is shifted down, for similar reasons.

The smaller model consistently predicts a smaller marginal mean.  The
relationship appears to be nearly perfectly linear, which is not too surprising,
since the smaller model differs by dropping two linear model terms that were 
averaged across by `emmeans()`.  

## By Year
```{r}
tibble(Year = by_year$Year,
       observed = avg_by_year,
       large = by_year$emmean,
       small = by_year_2$emmean ) %>%

ggplot(aes(x = observed)) +
  geom_point(aes(y = small), color = 'red') +
  geom_point(aes(y = large), color = 'blue') +
  geom_text(aes(y = small, label = Year),
            hjust = 0, nudge_x = 0.05, nudge_y = -0.05) +
  geom_abline(slope = 1, intercept = 0) +
  xlab('Observed') +
  ylab('Marginal Means') # +
 # xlim(6.5, 9.5) +
 # ylim(6.5,9.5)

```

Here, correlations between observed averages and estimated marginal means  are a
bit more consistent, with the exception of the truly wild value forecast for
Year 2010 from the "full" model.

Year 2010 is an outlier regarding seasonal availability of data.  Data 
collection began in June, so there is no data from the cooler months
of March, April, and May.  Apparently, the larger model makes a very large 
correction for the lack of data from those cool months.

Again, the smaller model consistently predicts the smaller marginal means.  This
probably reflects the impact of estimating marginal means by month, which is
not a term in the smaller model.

# Hierarchical Analysis of Trends

We hierarchical GAMs that includes both autocorrelated errors and a random
term by year.  The concept is that year to year variation can be thought of
as random based on annual weather, or perhaps watershed flow conditions. We test
for a long term trend against that random term, to minimize the risk that we
overinterpret year to year variability as a trend.  But note that this model
also includes terms for stream water temperature and flow.

## Model 1 : Site by Year interaction
We should be careful, as data is only available for selected years for three of
our sites, including SO5, S06B and S17.  This means we may be overfitting the
trends fror some of those sites based on a limited number of years.

We thought this would be a  slow model to fit, so we save a version, but the
model converges relatively rapidly.
```{r trend_gamm_1, cache = TRUE}
if (! file.exists("models/do_gamm_trend_1.rds")) {
  print(
    system.time(
      do_gamm_trend_1 <- gamm(DO_Median ~ Site * Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_trend_1, file="models/do_gamm_trend_1.rds")
} else {
  do_gamm_trend_1 <- readRDS("models/do_gamm_trend_1.rds")
}
```

### ANOVA
```{r}
anova(do_gamm_trend_1$gam)
```

Here the Site by Year term is marginally statistically significant, with both
the marginally significant Site and Site:Year interaction tied to S05.

### Summary
```{r}
summary(do_gamm_trend_1$gam)
```

### Estimated Daily Autocorrelation
```{r}
summary(do_gamm_trend_1$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r}
plot(do_gamm_trend_1$gam)
```


### Examine Marginal Means
We need to look at the marginally significant interaction, but we should be
careful here, as data is only available for selected years for three of our
sites, including SO5, S06B and S17.  
```{r}
the_call <-  quote(gamm(DO_Median ~ Site * Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_trend_1$gam$call <- the_call

#my_ref_grid <- ref_grid(do_gamm_trend_1, cov.keep = 'Year', cov.reduce = median) 
#(by_year <- summary(emmeans(my_ref_grid, ~ Site + Year )))
```

```{r}
emmip(do_gamm_trend_1, Site ~ Year,
      cov.keep = 'Year', cov.reduce = median,
      type = 'response') +
  ylab('Predicted DO Concentration')
```

That suggests that Site S05 has a low average dissolved oxygen, and a steep
decline in DO over time.  In fact, that is mostly an artifact of overfitting 
linear terms to a short record.  S05 data is only available from early in the 
period of record, and had a moderate DO record. 

```{r}
full_data %>%
  filter(Site == 'S05', ! is.na(DO_Median)) %>%
  group_by(Year) %>%
  summarize(n = n(),
            do_mean = mean(DO_Median),
            do_median = median(DO_Median))
```
So we fit a slope to a four year record, where a linear model makes effectively
no sense.

We conclude that the full interaction model is problematic.

## Model 2: No Interaction 
This model does slight violence to the prior analysis, but is arguably a better
description of what we know from the available data. It avoids overfitting the
short site by site records.
```{r trend_gamm_2, cache = TRUE}
if (! file.exists("models/do_gamm_trend_2.rds")) {
  print(
    system.time(
      do_gamm_trend_2 <- gamm(DO_Median ~ Site + Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_trend_2, file="models/do_gamm_trend_2.rds")
} else {
  do_gamm_trend_2 <- readRDS("models/do_gamm_trend_2.rds")
}
```

### ANOVA
```{r}
anova(do_gamm_trend_2$gam)
```
Here the Year term AND the Site terms are statistically significant.

## Summary
```{r}
summary(do_gamm_trend_2$gam)
```

## Estimated Daily Autocorrelation
```{r}
summary(do_gamm_trend_2$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r}
plot(do_gamm_trend_2$gam)
```

### Examine Marginal Means
We need to look at the marginally significant interaction, but we should be
careful here, as data is only available for selected years for three of our
sites, including SO5, S06B and S17.  
```{r}
the_call <-  quote(gamm(DO_Median ~ Site + Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_trend_2$gam$call <- the_call

my_ref_grid <- ref_grid(do_gamm_trend_2, cov.reduce = median, 
                        at = list(Year = 2014)) 
(by_site <- summary(emmeans(my_ref_grid, ~ Site  )))
```

```{r}
plot(by_site) +
  xlab('Predicted DO (mg/l)') +
  coord_flip() 
```

Note that we STILL predict low DO for S05 in 2014, but the prediction is
actually not far of the observed averages.



