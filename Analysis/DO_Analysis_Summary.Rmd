---
title: "Summary of Analysis of LCWMD Dissolved Oxygen Data"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "Revised 11/02/2021"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 4
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
This R Notebook explores several models for analyzing dissolved oxygen levels in 
Long Creek. The focus is on GAM models and their mixed model cousins, GAMMs.
All models are fit with autocorrelated error terms.

## Weather-Corrected Marginal Means
We use a (relatively) simple GAM model, developed in the "DO Analysis.rmd" 
notebook available in the more complete data analysis
[archive](https://github.com/CBEP-SoCB-Details/LCWMD_Monitoring).
to evaluate sources of variation in dissolved oxygen and extract (temperature
corrected) marginal means.

We examined significantly more complex models, but we prefer
model simplicity for the sake of communication to State of Casco Bay audiences.

We use a model of the form:

$$ \text{Oxygen} = f(\text{Covariates}) + g(\text{Predictors}) + \text{Error}$$

Where:
*  Covariates enter into the model either via linear functions or spline
   smoothers. Predictors include:
   -- natural log of the depth of water (in meters) at Site S05, in 
      mid-watershed, and 
   -- the local daily median water temperature at each monitoring Site.
   
*  The predictors enter the model via linear functions of:  
   --  site,  
   --  year,  
   --  time of year (Month in the models we consider here)
   
* The error includes both  *i.i.d* normal error and an AR(1) autocorrelated 
  error.

# Import Libraries  
```{r libraries}
library(tidyverse)
library(readr)

library(emmeans) # Provides tools for calculating marginal means
#library(nlme)

library(mgcv)    # generalized additive models. Function gamm() allows
                 # autocorrelation.

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

# Data Preparation
## Initial Folder References
```{r folders}
sibfldnm    <- 'Data'
parent      <- dirname(getwd())
sibling     <- file.path(parent,sibfldnm)

dir.create(file.path(getwd(), 'models'), showWarnings = FALSE)
```

## Load Data on Sites and Impervious Cover
These data were derived from Table 2 from a GZA report to the Long Creek
Watershed Management District, titled "Re: Long Creek Watershed Data Analysis;
Task 2: Preparation of Explanatory and Other Variables."  The Memo is dated
November 13, 2019 File No. 09.0025977.02.

Cumulative Area and IC calculations are our own, based on the GZA data and the
geometry of the stream channel.

```{r ic_data}
# Read in data and drop the East Branch, where we have no data
fn <- "Site_IC_Data.csv"
fpath <- file.path(sibling, fn)

Site_IC_Data <- read_csv(fpath) %>%
  filter(Site != "--") 

# Now, create a factor that preserves the order of rows (roughly upstream to downstream). 
Site_IC_Data <- Site_IC_Data %>%
  mutate(Site = factor(Site, levels = Site_IC_Data$Site))

# Finally, convert percent covers to numeric values
Site_IC_Data <- Site_IC_Data %>%
  mutate(CumPctIC = as.numeric(substr(CumPctIC, 1, nchar(CumPctIC)-1))) %>%
  mutate(PctIC = as.numeric(substr(PctIC, 1, nchar(PctIC)-1)))
Site_IC_Data
```

## Load Main Data
Read in the data from the Derived Data folder.

Note that we filter out data from 2019 because that is only a partial year,
which might affect estimation of things like seasonal trends.  We could add it
back in, but with care....

```{r main_data}
fn <- "Full_Data.csv"
fpath <- file.path(sibling, fn)

full_data <- read_csv(fpath, 
    col_types = cols(DOY = col_integer(), 
        D_Median = col_double(), Precip = col_number(), 
        X1 = col_skip(), Year = col_integer())) %>%
  mutate(Site = factor(Site, levels=levels(Site_IC_Data$Site)),
         Month = factor(Month, levels = month.abb),
         Year_f = factor(Year),
         IC=as.numeric(Site_IC_Data$CumPctIC[match(Site, Site_IC_Data$Site)]))
```

### Cleanup
```{r cleanup}
rm(Site_IC_Data)
rm(fn, fpath, parent, sibling, sibfldnm)
```

## Data Corrections
### Anomalous Depth Values
Several depth observations in the record appear highly unlikely. In particular,
several observations show daily median water depths over 15 meters. And
those observations were recorded in May or June, at site S05, with no associated 
record of significant precipitation, and no elevated depths at other sites on 
the stream.

We can trace these observations back to the raw QA/QC'd pressure and sonde data 
submitted to LCWMD by GZA, so they are not an artifact of our data preparation.

A few more observations show daily median depths over 4 meters, which also
looks unlikely in a stream of this size.  All these events also occurred in 
May or June of 2015 at site S05. Some sort of malfunction of the pressure 
transducer appears likely.

We remove these extreme values.  The other daily medians in May and June of 2015
appear reasonable, and we leave them in place, although given possible 
instability of the pressure sensors, it might make sense to remove them all.
```{r depth_data_cleanup}
full_data <- full_data %>%
  mutate(D_Median = if_else(D_Median > 4, NA_real_, D_Median),
         lD_Median = if_else(D_Median > 4, NA_real_, lD_Median))
```

### Single S06B Chloride Observation from 2017
The data includes just a single chloride observation from site S06B from
any year other than 2013.  While we do not know if the data point is legitimate
or not, it has very high leverage in several models, and we suspect a 
transcription error of some sort.
```{r show_bad_obs, fig.width = 3, fig.height = 2}
full_data %>%
  filter(Site == 'S06B') %>%
  select(sdate, DO_Median) %>%
  ggplot(aes(x = sdate, y = DO_Median)) + geom_point()
```

We remove the Chloride value from the data.
```{r fix_bad_obs}
full_data <- full_data %>%
  mutate(Chl_Median = if_else(Site == 'S06B' & Year > 2014,
                              NA_real_, Chl_Median))
```

### Anomolous Dissolved Oxygen and Chloride Values
#### Site S03, end of 2016
We noted some extreme dissolved oxygen data at the end of 2016.  Values were
both extreme and highly variable.
```{r show_bad_do_chl_data}
full_data %>% 
  filter (Year == 2016, Site == 'S03', Month %in% c("Oct", "Nov", "Dec")) %>%
ggplot(aes(x = sdate)) + 
  geom_point(aes(y = DO_Median)) +
  geom_line(aes(y = DO_Median)) +
  geom_line(aes(y = D_Median * 20), color = 'blue', lty = 2) +
  geom_line(aes(y = Chl_Median / 20), color = 'green', lty = 2) +
  geom_line(aes(y = MaxT), color = 'red', lty = 2) +
  theme_cbep(base_size = 10) +
  theme(legend.position="bottom", legend.box = "vertical") +
  
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 4, 
           label = 'Max Air Temp', color = 'red') +
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 2, 
           label = 'Depth x 20', color = 'blue') +
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 0, 
           label = 'Chlorides / 20', color = 'green')
```

The unstable behavior after October 23rd is questionable.  DO does not normally
fluctuate so widely. Percent saturation associated with these observations
extend as high as 200%.  There is clearly a problem.

We looked at the raw data, and examined the time course of all sonde-related data.

*  There was a sudden decline in observed DO at about 1:00 am on October 23rd,
   2016. That corresponds to a simultaneous rise in conductivity / chlorides,
   and follows a brief rise and rapid fall in water temperature.  
*  Recovery in DO several days later (1/28/2016) corresponds to a drop in
   chlorides, and a BREIF increase on water depth, but there is no related
   change in temperature.  
*  Ongoing brief spikes in DO appear to correspond to drops in chlorides
   or conductivity, and very brief small blips in water depth.  
*  The data record includes stable data at lower water depths, so it is unlikely
   that the sensors were exposed.  On the other hand, they could have been
   buried by sediment.  Air temperatures were not low enough to suggest that the
   sensors may have frozen, or battery power may have been failing.
   
The raw data makes it clear that whatever was going on affected both
conductivity and dissolved oxygen measurements, but did not obviously affect
temperature or pressure (water depth).  There are two possible interpretations.
Either the data are real, and those exceptionally high DO and percent saturation
values are real, or there was some sort of malfunction that affected both
chloride and dissolved oxygen.

We decide we should remove chloride and oxygen observations after October 15th.
Site S03 shows some low dissolved oxygen, high chloride observations from
November of 2015 as well, but the raw data is smoother, without the extreme high 
values, and without zero DO observations.  We leave those data in place as 
likely correct.

```{r fix_bad_do_chl_data}
full_data <- full_data %>% 
  mutate(Chl_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, Chl_Median),
         DO_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, DO_Median),
         PctSat_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, PctSat_Median))
```


## Remove Partial Data from Winter Months
We have very limited data from several months.  We have January data 
from only one year, and February data from only three, and December data from 
four, all older.  Both March and November sample sizes vary.

The limited winter data generates severely unbalanced samples, which may lead to
estimation problems, especially in models with crossed or potentially crossed
factors and predictors.  More fundamentally, the potential bias introduced by
showing data from those months from just a handful of years could give a
misleading impression of seasonal patterns.  We trim December, January and
February data, but leave the other months.

It is important to remember,  even after trimming the data, that:  
1.  2010 is a partial year,  
2.  The period of sampling in March may be biased due to spring melt timing.

```{r month_by_year_crosstab}
xtabs(~ Year_f + Month, data = full_data)
```

```{r remove_limited_winter_data}
full_data <- full_data %>%
  filter(Month %in% month.abb[3:11]  )
```

## Add Stream Flow Index
We worked through many models on a site by site basis in which we included data
on water depth, but since the depth coordinate is site-specific, a 10 cm depth
at one site may be exceptional, while at another it is commonplace. We generally
want not a local measure of stream depth, but a watershed-wide metric of high,
medium, or low stream flow.

Middle and Lower Maine Stem sites would be suitable for a general flow indicator
across the watershed. The monitoring sites in that stretch of Long Creek include
include S05 and S17, however only site S05 has been in continuous operation
throughout the period of record, so we use depth data from S05 to construct
our general stream flow indicator.

Stream flow at S05 is correlated with flow at other sites, although not all that
closely correlated to flow in the downstream tributaries.

```{r flow_correlations}
full_data %>%
  select(sdate, Site, lD_Median) %>%
  pivot_wider(names_from = Site, values_from = lD_Median) %>%
  select(-sdate) %>%
  cor(use = 'pairwise', method = 'pearson')
```

We use the log of the daily median flow at S05 as a general watershed-wide
stream flow indicator, which we call `FlowIndex`.  We use the log of the raw
median, to lessen the effect of the highly skewed distribution of stream depths
on the metric.

```{r make_flow_index}
depth_data <- full_data %>%
  filter (Site == 'S05') %>%
  select(sdate, lD_Median)

full_data <- full_data %>%
  mutate(FlowIndex = depth_data$lD_Median[match(sdate, depth_data$sdate)])
  rm(depth_data)
```

Note that because the flow record at S05 has some gaps, any model using this
predictor is likely to have a smaller sample size.

## Select Final Data Set
```{r final_data}
full_data <- full_data %>%
  mutate(Year_f = factor(Year)) %>%
  select (Site, sdate, Year, Year_f, Month, DOY, 
          Precip, lPrecip, wlPrecip, MaxT,
          D_Median, lD_Median,
          DO_Median, PctSat_Median, T_Median, Chl_Median,
          IC, FlowIndex) %>%
  filter(! is.na(DO_Median))
```

# GAMM Analysis
Here we use "General Additive Models" that allow non-linear (smoother) fits for
some parameters. Our emphasis is on using smoothers to account for
non-linearities in relationships between weather or flow-related predictors and
dissolved oxygen.

We use the function `gamm()` because it has a relatively simple interface for
incorporating autocorrelated errors.

We abuse the autocorrelation model slightly, since we don't fit
separate autocorrelations for each site and season. That should have little
impact on results, as missing values at beginning and end of most time series
prevent estimation anyway.

## Initial GAMM model
This model is takes approximately 15 minutes to run.  Model selection was based
on preliminary exploration of several other GLS, GAM and GAMM models (not 
shown).  
```{r first_gamm, cache = TRUE}
if (! file.exists("models/do_gamm.rds")) {
  print(
    system.time(
      do_gamm <- gamm(DO_Median ~ Site + 
                        T_Median +
                        s(FlowIndex) +
                        Month +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm, file="models/do_gamm.rds")
} else {
  do_gamm <- readRDS("models/do_gamm.rds")
}
```

### ANOVA
```{r anova_first_gamm}
anova(do_gamm$gam)
```

We can conclude that all the terms in the model are important, but we need to be
careful interpreting the results, as sampling history varied site to site.

### Summary
```{r summary_first_gamm}
summary(do_gamm$gam)
```

Again, all predictors and covariates are important.  We look more closely at the
implied patterns by looking at "marginal means".  Marginal means (also known as
"least squares Means" in SAS) are predicted means based on a specific model.
They are "marginal" because they are calculated by ignoring certain covariates,
usually either by calculating predicted values holding other predictors fixed, otr
averaging predictors across covariates.

### Estimated Daily Autocorrelation
```{r autocor_first_gamm}
summary(do_gamm$lme)$modelStruct$corStruct
```
Median daily dissolved oxygen values are highly autocorrelated.

### Structure of the Smoother
```{r smoother_first_gamm}
plot(do_gamm$gam)
```

Dissolved oxygen is related to stream flow, but in a non-linear way. Under low
to moderate stream flows, dissolved oxygen is correlated with flow, climbing
rapidly with flow.  But when flow is high, dissolved oxygen levels are unrelated
to flow.

### Diagnostic Plots
The help files for `gam.check()` suggest using care when interpreting results
for GAMM models, since the function does not correctly incorporate the error
correlation structure.  However, for our purposes, this is probably sufficient,
since our focus is not on statistical significance, but on estimation.
```{r diagnostics_first_gamm}
gam.check(do_gamm$gam)
```
What that shows, unfortunately, is that the extreme low DO events -- which are
our highest priority in many ways -- are rather poorly modeled.  And it is clear
the assumptions of normality are not met, especially for those low values.

For careful work, we should use bootstrapped confidence intervals, but given how 
long these models take to fit, that is not practical.  Besides, it is probably
overkill for our needs, as relationships are generally highly significant and
sample sizes large.

### Estimated Marginal Means
Calling `emmeans()` for `gamm()` models requires 
creating a call object and associating it with the model (e.g., as
`do_gamm$gam$call`). (See the `emmeans` "models" vignette for more info,
although not all strategies recommended there worked for us).

We first create the call object, then associate it with the model, and finally
manually construct a reference grid before calling `emmeans()` to extract
marginal means.  This workflow has the advantage that it requires us to think
carefully about the structure of the reference grid.

The default `emmeans()` behavior creates a reference grid where marginal 
means are keyed to mean values of all quantitative predictors, but averaged
across all factors.  Since we fit Year only as a factor, we do not specify year 
here.

#### By Month
```{r ls_means_month_first_gamm}
the_call <-  quote(gamm(DO_Median ~ Site + 
                        T_Median +
                        s(FlowIndex) +
                        Month +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm$gam$call <- the_call

my_ref_grid <- ref_grid(do_gamm, cov.reduce = median) 
(by_month <- summary(emmeans(my_ref_grid, ~ Month)))
```

```{r plot_ls_means_month_first_GAMM}
labl <- 'Values Adjusted for Flow and\nWater Temperature\nAll Sites Combined'

plot(by_month) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab ('') +
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```

While marginal means (across all sites) are above thresholds, there is a clear 
seasonal pattern with lower dissolved oxygen in the warmer months (as expected).
Note that these marginal means are adjusted for temperature and stream flow.

#### By Site
```{r calc_emm_site_first_GAMM}
(by_site <- summary(emmeans(my_ref_grid, ~ Site)))
```

```{r plot_ls_means_site_first_GAMM, fig.width = 5, fig.height = 4}
plot(by_site) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab("Upstream         Main Stem          Lower Tribs") +
  #annotate('text', 11, 2.5, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```

Note the relatively high (adjusted) marginal means. These represent year-round
averages, and do not reflect seasonal patterns. Several sites often have low
summer dissolved oxygen levels, especially under low flow conditions, but those
details are buried here looking only at marginal means.

#### By Year
```{r}
my_ref_grid <- ref_grid(do_gamm, cov.reduce = median) 
by_year <- summary(emmeans(my_ref_grid, 'Year_f'))
by_year
```

```{r plot_ls_means_year_first_GAMM}
plot(by_year) + 
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab('') +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```
2010 was a partial year, so despite (or perhaps because of) adjusting for
months, the 2010 estimate may be misleading.  Since then, basically, 2016 is 
way worse than the other years.


# Hierarchical Analysis of Trends
We develop hierarchical GAMs that includes both autocorrelated errors and a
random term by year.  The concept is that year to year variation can be thought
of as random based on annual weather, or perhaps watershed flow conditions. An
alternative way of describing the same idea is that observations made during
the same year tend to be more highly correlated than observations from separate 
years.

We test for a long term trend against that random term, to minimize the risk
that we overinterpret year to year variability as a trend.  But note that this
model also includes terms for stream water temperature and flow.

## Model 1 : Site by Year interaction
We should be careful, as data is only available for selected years for three of
our sites, including SO5, S06B and S17.  This means we may be overfitting the
trends fror some of those sites based on a limited number of years.

We thought this would be a  slow model to fit, so we save a version, but the
model converges relatively rapidly.
```{r trend_gamm_1, cache = TRUE}
if (! file.exists("models/do_gamm_trend_1.rds")) {
  print(
    system.time(
      do_gamm_trend_1 <- gamm(DO_Median ~ Site * Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_trend_1, file="models/do_gamm_trend_1.rds")
} else {
  do_gamm_trend_1 <- readRDS("models/do_gamm_trend_1.rds")
}
```

### ANOVA
```{r anova_trend_GAMM_1}
anova(do_gamm_trend_1$gam)
```

Here the Site by Year term is marginally statistically significant, with both
the marginally significant Site and Site:Year interaction tied to S05.

### Summary
```{r summary_trend_GAMM_1}
summary(do_gamm_trend_1$gam)
```

### Estimated Daily Autocorrelation
```{r autocor_trend_GAMM_1}
summary(do_gamm_trend_1$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r smoother_trend_GAMM_1}
plot(do_gamm_trend_1$gam)
```

### Examine Marginal Means
We need to look at the marginally significant interaction, but we should be
careful here, as data is only available for selected years for three of our
sites, including SO5, S06B and S17.  
```{r maemms_trend_GAMM_1}
the_call <-  quote(gamm(DO_Median ~ Site * Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_trend_1$gam$call <- the_call

#my_ref_grid <- ref_grid(do_gamm_trend_1, cov.keep = 'Year', cov.reduce = median) 
#(by_year <- summary(emmeans(my_ref_grid, ~ Site + Year )))
```

```{r plot)emms_trend_GAMM_1}
emmip(do_gamm_trend_1, Site ~ Year,
      cov.keep = 'Year', cov.reduce = median,
      type = 'response') +
  ylab('Predicted DO Concentration')
```

That suggests that Site S05 has a low average dissolved oxygen, and a steep
decline in DO over time.  In fact, that is mostly an artifact of overfitting 
linear terms to a short record.  S05 data is only available from early in the 
period of record, and had a moderate DO record. 

```{r show_S05_data}
full_data %>%
  filter(Site == 'S05', ! is.na(DO_Median)) %>%
  group_by(Year) %>%
  summarize(n = n(),
            do_mean = mean(DO_Median),
            do_median = median(DO_Median))
```
So we basically just fit a slope to a four year record, where a linear model 
effectively no sense.

We conclude that the full interaction model is problematic, and go on to a
reduced model that omits the interaction terms.

## Model 2: No Interaction 
This model is *also* problematic as there are likely to be differences
between sites. The prior analysis suggests site by year interactions, but they
can not be interpreted because of the different sampling histories at each site.
This model avoids overfitting the short records at some sites. Our goal
is to look for long-terms trends in DO, and that question can be examined
across all sites.
```{r trend_gamm_2, cache = TRUE}
if (! file.exists("models/do_gamm_trend_2.rds")) {
  print(
    system.time(
      do_gamm_trend_2 <- gamm(DO_Median ~ Site + Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_trend_2, file="models/do_gamm_trend_2.rds")
} else {
  do_gamm_trend_2 <- readRDS("models/do_gamm_trend_2.rds")
}
```

### ANOVA
```{r anova_trend_gamm_2}
anova(do_gamm_trend_2$gam)
```
Here the Year term is statistically significant, suggesting a long term decline
in dissolved oxygen, even after accounting for changes in stream temperature
and the stream-wide flow index.

### Summary
```{r summary_trend_gamm_2}
summary(do_gamm_trend_2$gam)
```

### Estimated Daily Autocorrelation
```{r autocor_trend_gamm_2}
summary(do_gamm_trend_2$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r smoother_trend_gamm_2}
plot(do_gamm_trend_2$gam)
```

The smoother looks nearly identical to the one fit in the full model.

### Examine Marginal Means
```{r emm_trend_gamm_2}
the_call <-  quote(gamm(DO_Median ~ Site + Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_trend_2$gam$call <- the_call

my_ref_grid <- ref_grid(do_gamm_trend_2, cov.reduce = median, 
                        at = list(Year = 2014)) 
(by_site <- summary(emmeans(my_ref_grid, ~ Site  )))
```

```{r plot_trend_gamm_2_quick}
plot(by_site) +
  xlab('Predicted DO (mg/l)') +
  coord_flip() 
```

Note that we STILL predict low DO for S05, but the prediction is not as far off 
the observed averages.
