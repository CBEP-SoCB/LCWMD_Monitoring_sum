---
title: "Summary of Analysis of LCWMD Dissolved Oxygen Data"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "Revised 11/02/2021"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 4
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Import Libraries  
```{r libraries}
library(tidyverse)
library(readr)

library(emmeans) # Provides tools for calculating marginal means
#library(nlme)

library(mgcv)    # generalized additive models. Function gamm() allows
                 # autocorrelation.

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

# Data Preparation
## Initial Folder References
```{r folders}
sibfldnm    <- 'Data'
parent      <- dirname(getwd())
sibling     <- file.path(parent,sibfldnm)

dir.create(file.path(getwd(), 'models'), showWarnings = FALSE)
```

## Load Weather Data
```{r load_weather_data}
fn <- "Portland_Jetport_2009-2019.csv"
fpath <- file.path(sibling, fn)

weather_data <- read_csv(fpath, 
 col_types = cols(.default = col_skip(),
        date = col_date(),
        PRCP = col_number(), PRCPattr = col_character() #,
        #SNOW = col_number(), SNOWattr = col_character(), 
        #TMIN = col_number(), TMINattr = col_character(), 
        #TAVG = col_number(), TAVGattr = col_character(), 
        #TMAX = col_number(), TMAXattr = col_character(), 
        )) %>%
  rename(sdate = date) %>%
  mutate(pPRCP = dplyr::lag(PRCP))
```

## Load Data on Sites and Impervious Cover
These data were derived from Table 2 from a GZA report to the Long Creek
Watershed Management District, titled "Re: Long Creek Watershed Data Analysis;
Task 2: Preparation of Explanatory and Other Variables."  The Memo is dated
November 13, 2019 File No. 09.0025977.02.

Cumulative Area and IC calculations are our own, based on the GZA data and the
geometry of the stream channel.

```{r ic_data}
# Read in data and drop the East Branch, where we have no data
fn <- "Site_IC_Data.csv"
fpath <- file.path(sibling, fn)

Site_IC_Data <- read_csv(fpath) %>%
  filter(Site != "--") 

# Now, create a factor that preserves the order of rows (roughly upstream to downstream). 
Site_IC_Data <- Site_IC_Data %>%
  mutate(Site = factor(Site, levels = Site_IC_Data$Site))

# Finally, convert percent covers to numeric values
Site_IC_Data <- Site_IC_Data %>%
  mutate(CumPctIC = as.numeric(substr(CumPctIC, 1, nchar(CumPctIC)-1))) %>%
  mutate(PctIC = as.numeric(substr(PctIC, 1, nchar(PctIC)-1)))
Site_IC_Data
```

## Load Main Data
Read in the data from the Derived Data folder.

Note that we filter out data from 2019 because that is only a partial year,
which might affect estimation of things like seasonal trends.  We could add it
back in, but with care....

```{r main_data}
fn <- "Full_Data.csv"
fpath <- file.path(sibling, fn)

full_data <- read_csv(fpath, 
    col_types = cols(DOY = col_integer(), 
        D_Median = col_double(), Precip = col_number(), 
        X1 = col_skip(), Year = col_integer())) %>%
  mutate(Site = factor(Site, levels=levels(Site_IC_Data$Site)),
         Month = factor(Month, levels = month.abb),
         Year_f = factor(Year),
         IC=as.numeric(Site_IC_Data$CumPctIC[match(Site, Site_IC_Data$Site)]))
```

### Cleanup
```{r cleanup}
rm(Site_IC_Data, weather_data)
rm(fn, fpath, parent, sibling, sibfldnm)
```

## Data Corrections
### Anomolous Depth Values
Several depth observations in the record appear highly unlikely. In particular,
several observations show daily median water depths over 15 meters. And
those observations were recorded in May or June, at site S05, with no associated 
record of significant precipitation, and no elevated depths at other sites on 
the stream.

We can trace these observations back to the raw QA/QC'd pressure and sonde data 
submitted to LCWMD by GZA, so they are not an artifact of our data preparation.

A few more observations show daily median depths over 4 meters, which also
looks unlikely in a stream of this size.  All these events also occurred in 
May or June of 2015 at site S05. Some sort of malfunction of the pressure 
transducer appears likely.

We remove these extreme values.  The other daily medians in May and June of 2015
appear reasonable, and we leave them in place, although given possible 
instability of the pressure sensors, it might make sense to remove them all.
```{r depth_data_cleanup}
full_data <- full_data %>%
  mutate(D_Median = if_else(D_Median > 4, NA_real_, D_Median),
         lD_Median = if_else(D_Median > 4, NA_real_, lD_Median))
```

### Single S06B Chloride Observation from 2017
The data includes just a single chloride observation from site S06B from
any year other than 2013.  While we do not know if the data point is legitimate
or not, it has very high leverage in several models, and we suspect a 
transcription error of some sort.
```{rshow_bad_obs, fig.width = 3, fig.height = 2}
full_data %>%
  filter(Site == 'S06B') %>%
  select(sdate, DO_Median) %>%
  ggplot(aes(x = sdate, y = DO_Median)) + geom_point()
```

We remove the Chloride value from the data.
```{r fix_bad_obs}
full_data <- full_data %>%
  mutate(Chl_Median = if_else(Site == 'S06B' & Year > 2014,
                              NA_real_, Chl_Median))
```

### Anomolous Dissolved Oxygen and Chloride Values
#### Site S03, end of 2016
We noted some extreme dissolved oxygen data at the end of 2016.  Values were
both extreme and highly variable.
```{r show_bad_do_chl_data}
full_data %>% 
  filter (Year == 2016, Site == 'S03', Month %in% c("Oct", "Nov", "Dec")) %>%
ggplot(aes(x = sdate)) + 
  geom_point(aes(y = DO_Median)) +
  geom_line(aes(y = DO_Median)) +
  geom_line(aes(y = D_Median * 20), color = 'blue', lty = 2) +
  geom_line(aes(y = Chl_Median / 20), color = 'green', lty = 2) +
  geom_line(aes(y = MaxT), color = 'red', lty = 2) +
  theme_cbep(base_size = 10) +
  theme(legend.position="bottom", legend.box = "vertical") +
  
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 4, 
           label = 'Max Air Temp', color = 'red') +
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 2, 
           label = 'Depth x 20', color = 'blue') +
  annotate(geom ='text', x = as.Date('2016-10-10'), y = 0, 
           label = 'Chlorides / 20', color = 'green')
```

The unstable behavior after October 23rd is questionable.  DO does not normally
fluctuate so widely. Percent saturation associated with these observations
extend as high as 200%.  There is clearly a problem.

We looked at the raw data, and examined the time course of all sonde-related data.

*  There was a sudden decline in observed DO at about 1:00 am on October 23rd,
   2016. That corresponds to a simultaneous rise in conductivity / chlorides,
   and follows a brief rise and rapid fall in water temperature.  
*  Recovery in DO several days later (1/28/2016) corresponds to a drop in
   chlorides, and a BREIF increase on water depth, but there is no related
   change in temperature.  
*  Ongoing brief spikes in DO appear to correspond to drops in chlorides
   or conductivity, and very brief small blips in water depth.  
*  The data record includes stable data at lower water depths, so it is unlikely
   that the sensors were exposed.  On the other hand, they could have been
   buried by sediment.  Air temperatures were not low enough to suggest that the
   sensors may have frozen, or battery power may have been failing.
   
The raw data makes it clear that whatever was going on affected both
conductivity and dissolved oxygen measurements, but did not obviously affect
temperature or pressure (water depth).  There are two possible interpretations.
Either the data are real, and those exceptionally high DO and percent saturation
values are real, or there was some sort of malfunction that affected both
chloride and dissolved oxygen.

We decide we should remove chloride and oxygen observations after October 15th.
Site S03 shows some low dissolved oxygen, high chloride observations from
November of 2015 as well, but the raw data is smoother, without the extreme high 
values, and without zero DO observations.  We leave those data in place as 
likely correct.

```{r fix_bad_do_chl_data}
full_data <- full_data %>% 
  mutate(Chl_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, Chl_Median),
         DO_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, DO_Median),
         PctSat_Median = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA_real_, PctSat_Median))
```


## Remove Partial Data from Winter Months
We have very limited data from several months.  We have January data 
from only one year, and February data from only three, and December data from 
four, all older.  Both March and November sample sizes vary.

The limited winter data generates severely unbalanced samples, which may lead to
estimation problems, especially in models with crossed or potentially crossed
factors and predictors.  More fundamentally, the potential bias introduced by
showing data from those months from just a handful of years could give a
misleading impression of seasonal patterns.  We trim December, January and
February data, but leave the other months.

It is important to remember,  even after trimming the data, that:  
1.  2010 is a partial year,  
2.  The period of sampling in March may be biased due to spring melt timing.

```{r month_by_year_crosstab}
xtabs(~ Year_f + Month, data = full_data)
```

```{r remove_limited_winter_data}
full_data <- full_data %>%
  filter(Month %in% month.abb[3:11]  )
```

## Add Stream Flow Index
We worked through many models on a site by site basis in which we included data
on water depth, but since the depth coordinate is site-specific, a 10 cm depth
at one site may be exceptional, while at another it is commonplace. We generally
want not a local measure of stream depth, but a watershed-wide metric of high,
medium, or low stream flow.

Middle and Lower Maine Stem sites would be suitable for a general flow indicator
across the watershed. The monitoring sites in that stretch of Long Creek include
include S05 and S17, however only site S05 has been in continuous operation
throughout the period of record, so we use depth data from S05 to construct
our general stream flow indicator.

Stream flow at S05 is correlated with flow at other sites, although not all that
closely correlated to flow in the downstream tributaries.

```{r flow_correlations}
full_data %>%
  select(sdate, Site, lD_Median) %>%
  pivot_wider(names_from = Site, values_from = lD_Median) %>%
  select(-sdate) %>%
  cor(use = 'pairwise', method = 'pearson')
```

We use the log of the daily median flow at S05 as a general watershed-wide
stream flow indicator, which we call `FlowIndex`.  We use the log of the raw
median, to lessen the effect of the highly skewed distribution of stream depths
on the metric.

```{r make_flow_index}
depth_data <- full_data %>%
  filter (Site == 'S05') %>%
  select(sdate, lD_Median)

full_data <- full_data %>%
  mutate(FlowIndex = depth_data$lD_Median[match(sdate, depth_data$sdate)])
  rm(depth_data)
```

Note that because the flow record at S05 has some gaps, any model using this
predictor is likely to have a smaller sample size.

## Select Final Data Set
```{r final_data}
full_data <- full_data %>%
  mutate(Year_f = factor(Year)) %>%
  select (Site, sdate, Year, Year_f, Month, DOY, 
          Precip, lPrecip, wlPrecip, MaxT,
          D_Median, lD_Median,
          DO_Median, PctSat_Median, T_Median, Chl_Median,
          IC, FlowIndex) %>%
  filter(! is.na(DO_Median))
```

# GAMM Analysis
Here we use "General Additive Models" that allow non-linear (smoother) fits for
some parameters. Our emphasis is on using smoothers to account for
non-linearities in relationships between weather or flow-related predictors and
dissolved oxygen.

We use the function `gamm()` because it has a relatively simple interface for
incorporating autocorrelated errors.

We abuse the autocorrelation model slightly, since we don't fit
separate autocorrelations for each site and season. That should have little
impact on results, as missing values at beginning and end of most time series
prevent estimation anyway.

## Initial GAMM model
This model is takes approximately 15 minutes to run.  Model selection was based
on preliminary exploration of several other GLS, GAM and GAMM models (not 
shown).  
```{r first_gamm, cache = TRUE}
if (! file.exists("models/do_gamm.rds")) {
  print(
    system.time(
      do_gamm <- gamm(DO_Median ~ Site + 
                        T_Median +
                        s(FlowIndex) +
                        Month +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm, file="models/do_gamm.rds")
} else {
  do_gamm <- readRDS("models/do_gamm.rds")
}
```

### ANOVA
```{r anova_first_gamm}
anova(do_gamm$gam)
```

### Summary
```{r summary_first_gamm}
summary(do_gamm$gam)
```

### Estimated Daily Autocorrelation
```{r autocor_first_gamm}
summary(do_gamm$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r smoother_first_gamm}
plot(do_gamm$gam)
```

### Diagnostic Plots
The help files for `gam.check()` suggest using care when interpreting results
for GAMM models, since the function does not correctly incorporate the error
correlation structure.  However, for our purposes, this is probably sufficient,
since our focus is not on statistical significance, but on estimation.
```{r diagnostics_first_gamm}
gam.check(do_gamm$gam)
```
What that shows, unfortunately, is that the extreme low DO events -- which are
our highest priority in many ways -- are rather poorly modeled.  And it is clear
the assumptions of normality are not met, especially for those low values.

For careful work, we should probably use bootstrapped confidence intervals or
something similar, but given how long these models take to fit, that is not 
practical.  Besides, it is probably overkill for our needs, as relationships
are generally highly significant and sample sizes large.

### Estimated Marginal Means
Reliably calling `emmeans()` for `gamm()` models requires 
creating a call object and associating it with the model (e.g., as
`do_gamm$gam$call`). (See the `emmeans` "models" vignette for more info,
although not all strategies recommended there worked for us).

We first create the call object, then associate it with the model, and finally
manually construct a reference grid before calling `emmeans()` to extract
marginal means.  This workflow has the advantage that it requires us to think
carefully about the structure of the reference grid.

The default `emmeans()` behavior creates a reference grid where marginal 
means are keyed to mean values of all quantitative predictors, but averaged
across all factors.  Since we fit Year only as a factor, we do not specify year 
here.

#### By Month
```{r ls_means_month_first_gamm}
the_call <-  quote(gamm(DO_Median ~ Site + 
                        T_Median +
                        s(FlowIndex) +
                        Month +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm$gam$call <- the_call

my_ref_grid <- ref_grid(do_gamm, cov.reduce = median) 
(by_month <- summary(emmeans(my_ref_grid, ~ Month)))
```

```{r plot_ls_means_month_first_GAMM}
labl <- 'Values Adjusted for Flow and\nWater Temperature\nAll Sites Combined'

plot(by_month) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab ('') +
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```

#### By Site
```{r calc_emm_site_first_GAMM}
(by_site <- summary(emmeans(my_ref_grid, ~ Site)))
```

```{r plot_ls_means_site_first_GAMM, fig.width = 5, fig.height = 4}
plot(by_site) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab("Upstream         Main Stem          Lower Tribs") +
  #annotate('text', 11, 2.5, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```

#### By Year
```{r}
my_ref_grid <- ref_grid(do_gamm, cov.reduce = median) 
by_year <- summary(emmeans(my_ref_grid, 'Year_f'))
by_year
```

```{r plot_ls_means_year_first_GAMM}
plot(by_year) + 
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab('') +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```
2010 was a partial year, so despite (or perhaps because of) adjusting for
months, the 2010 estimate may be misleading.  Since then, basically, 2016 is 
way worse than the other years.

## Simplified Model
We drop the MONTH term and the FLOW term.  We refit the water temperature term
as a low dimensional smooth, because we need to include at least one smoother in
the GAMM model.
```{r second_gamm, cache = TRUE}
if (! file.exists("models/do_gamm_2.rds")) {
  print(
    system.time(
      do_gamm_2<- gamm(DO_Median ~ Site + 
                        s(T_Median, k = 1) +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_2, file="models/do_gamm_2.rds")
} else {
  do_gamm_2 <- readRDS("models/do_gamm_2.rds")
}
```

### ANOVA
```{r anova_second_GAMM}
anova(do_gamm_2$gam)
```

### Summary
```{r sumamry_second_GAMM)}
summary(do_gamm_2$gam)
```

### Estimated Daily Autocorrelation
```{r autoco_second_GAMM}
summary(do_gamm_2$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r smoother_second_GAMM}
plot(do_gamm_2$gam)
```

### Diagnostic Plots
```{r diagnostics_second_GAMM}
gam.check(do_gamm_2$gam)
```
The model has essentially the same inadequacies as the prior model.

### Estimated Marginal Means
We again create the call object, and associate it with the model, and finally
manually construct a reference grid before calling `emmeans()` to extract
marginal means.  We explicitly specify that we want the marginal means 
estimated at Year = 2014.
```{r ls_means_month_second_gamm}
the_call <-  quote(gamm(DO_Median ~ Site + 
                        s(T_Median, k = 1) +
                        Year_f,
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_2$gam$call <- the_call
my_ref_grid <- ref_grid(do_gamm_2, cov.reduce = median) 
```

#### By Site
```{r calc_emms_site_second_gamm}
(by_site_2 <- summary(emmeans(my_ref_grid, ~ Site)))
```

```{r plot_emms_site_second_gamm, fig.width = 5, fig.height = 4}
plot(by_site_2) + 
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab("Upstream         Main Stem          Lower Tribs") +
  #annotate('text', 11, 2.5, label = labl, size = 3) +
  xlim(0,12) +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```

#### By Year
```{r  calc_emms_year_second_gamm}
my_ref_grid <- ref_grid(do_gamm_2, cov.reduce = median) 
(by_year_2 <- summary(emmeans(my_ref_grid, 'Year_f')))
```

```{r plot_emms_year_second_gamm, fig.width = 5, fig.height = 4}
plot(by_year_2) + 
  annotate('text', 11, 6, label = labl, size = 3) +
  xlim(0,12) +
  xlab('DO (mg/l)\n(Flow and Temperature Adjusted)') +
  ylab('') +
  geom_vline(xintercept =  7, color = 'orange') +
  geom_vline(xintercept =  5, color = 'red') +
  coord_flip() +
  theme_cbep(base_size = 12)
```

# Compare Marginal Means From Two Models
## Calculate Observed Averages
```{r calc_observed_marginal_means}
avg_by_site <- full_data %>%
  select(Site, DO_Median) %>%
  group_by(Site) %>%
  summarize(observed = mean(DO_Median, na.rm = TRUE),
            .groups = 'drop')  %>%
  pull(observed)

avg_by_year <- full_data %>%
  select(Year_f, DO_Median) %>%
  group_by(Year_f) %>%
  summarize(observed = mean(DO_Median, na.rm = TRUE),
            .groups = 'drop')  %>%
  pull(observed)
```

## By Site
```{r plot_compare_mods_to_obs_site}
tibble(Site = by_site$Site,
       observed = avg_by_site,
       large = by_site$emmean,
       small = by_site_2$emmean ) %>%

ggplot(aes(x = observed)) +
  geom_point(aes(y = small), color = 'red') +
  geom_point(aes(y = large), color = 'blue') +
  geom_text(aes(y = small, label = Site),
            hjust = 0, nudge_x = 0.05, nudge_y = -0.05) +
  geom_abline(slope = 1, intercept = 0) +
  xlab('Observed') +
  ylab('Marginal Means')  +
  xlim(6.5, 9.5) +
  ylim(6.5,9.5)

```

Correspondence with observed means is only so-so, as is expected with uneven
sampling histories. The main difference is in the positions of S05 and S17.
These sites have inconsistent sampling histories, so marginal means are adjusted 
by year.  S17 was observed principally during "bad" years, so the marginal mean
(which is averaged across for ALL years) as adjusted upwards, since the model
concludes the observed values would probably have been better.  Meanwhile, site 
S05 is shifted down, for similar reasons.

The smaller model consistently predicts a smaller marginal mean. The
relationship appears to be nearly perfectly linear, which is not too surprising,
since the smaller model differs by dropping two linear model terms that were 
averaged across by `emmeans()`.  

## By Year
```{r plot_compare_mods_to_obs_year}
tibble(Year = by_year$Year,
       observed = avg_by_year,
       large = by_year$emmean,
       small = by_year_2$emmean ) %>%

ggplot(aes(x = observed)) +
  geom_point(aes(y = small), color = 'red') +
  geom_point(aes(y = large), color = 'blue') +
  geom_text(aes(y = small, label = Year),
            hjust = 0, nudge_x = 0.05, nudge_y = -0.05) +
  geom_abline(slope = 1, intercept = 0) +
  xlab('Observed') +
  ylab('Marginal Means') # +
 # xlim(6.5, 9.5) +
 # ylim(6.5,9.5)

```

Here, correlations between observed averages and estimated marginal means  are a
bit more consistent, with the exception of the truly wild value forecast for
Year 2010 from the "full" model.

Year 2010 is an outlier regarding seasonal availability of data.  Data 
collection began in June, so there is no data from the cooler months
of March, April, and May.  Apparently, the larger model makes a very large 
correction for the lack of data from those cool months.

Again, the smaller model consistently predicts the smaller marginal means.  This
probably reflects the impact of estimating marginal means by month, which is
not a term in the smaller model.

# Hierarchical Analysis of Trends
We develop hierarchical GAMs that includes both autocorrelated errors and a
random term by year.  The concept is that year to year variation can be thought
of as random based on annual weather, or perhaps watershed flow conditions. We
test for a long term trend against that random term, to minimize the risk that
we overinterpret year to year variability as a trend.  But note that this model
also includes terms for stream water temperature and flow.

## Model 1 : Site by Year interaction
We should be careful, as data is only available for selected years for three of
our sites, including SO5, S06B and S17.  This means we may be overfitting the
trends fror some of those sites based on a limited number of years.

We thought this would be a  slow model to fit, so we save a version, but the
model converges relatively rapidly.
```{r trend_gamm_1, cache = TRUE}
if (! file.exists("models/do_gamm_trend_1.rds")) {
  print(
    system.time(
      do_gamm_trend_1 <- gamm(DO_Median ~ Site * Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_trend_1, file="models/do_gamm_trend_1.rds")
} else {
  do_gamm_trend_1 <- readRDS("models/do_gamm_trend_1.rds")
}
```

### ANOVA
```{r anova_trend_GAMM_1}
anova(do_gamm_trend_1$gam)
```

Here the Site by Year term is marginally statistically significant, with both
the marginally significant Site and Site:Year interaction tied to S05.

### Summary
```{r summary_trend_GAMM_1}
summary(do_gamm_trend_1$gam)
```

### Estimated Daily Autocorrelation
```{r autocor_trend_GAMM_1}
summary(do_gamm_trend_1$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r smoother_trend_GAMM_1}
plot(do_gamm_trend_1$gam)
```

### Examine Marginal Means
We need to look at the marginally significant interaction, but we should be
careful here, as data is only available for selected years for three of our
sites, including SO5, S06B and S17.  
```{r maemms_trend_GAMM_1}
the_call <-  quote(gamm(DO_Median ~ Site * Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_trend_1$gam$call <- the_call

#my_ref_grid <- ref_grid(do_gamm_trend_1, cov.keep = 'Year', cov.reduce = median) 
#(by_year <- summary(emmeans(my_ref_grid, ~ Site + Year )))
```

```{r plot)emms_trend_GAMM_1}
emmip(do_gamm_trend_1, Site ~ Year,
      cov.keep = 'Year', cov.reduce = median,
      type = 'response') +
  ylab('Predicted DO Concentration')
```

That suggests that Site S05 has a low average dissolved oxygen, and a steep
decline in DO over time.  In fact, that is mostly an artifact of overfitting 
linear terms to a short record.  S05 data is only available from early in the 
period of record, and had a moderate DO record. 

```{r show_S05_data}
full_data %>%
  filter(Site == 'S05', ! is.na(DO_Median)) %>%
  group_by(Year) %>%
  summarize(n = n(),
            do_mean = mean(DO_Median),
            do_median = median(DO_Median))
```
So we fit a slope to a four year record, where a linear model makes effectively
no sense.

We conclude that the full interaction model is problematic.

## Model 2: No Interaction 
This model does slight violence to the prior analysis, but is arguably a better
description of what we know from the available data. It avoids overfitting the
short records at two sites.
```{r trend_gamm_2, cache = TRUE}
if (! file.exists("models/do_gamm_trend_2.rds")) {
  print(
    system.time(
      do_gamm_trend_2 <- gamm(DO_Median ~ Site + Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data)
    )
  )
  saveRDS(do_gamm_trend_2, file="models/do_gamm_trend_2.rds")
} else {
  do_gamm_trend_2 <- readRDS("models/do_gamm_trend_2.rds")
}
```

### ANOVA
```{r anova_trend_gamm_2}
anova(do_gamm_trend_2$gam)
```
Here the Year term AND the Site terms are statistically significant.

### Summary
```{r summary_trend_gamm_2}
summary(do_gamm_trend_2$gam)
```

### Estimated Daily Autocorrelation
```{r autocor_trend_gamm_2}
summary(do_gamm_trend_2$lme)$modelStruct$corStruct
```

### Structure of the Smoother
```{r smoother_trend_gamm_2}
plot(do_gamm_trend_2$gam)
```

### Examine Marginal Means
We need to look at the marginally significant interaction, but we should be
careful here, as data is only available for selected years for three of our
sites, including SO5, S06B and S17.  
```{r emm_trend_gamm_2}
the_call <-  quote(gamm(DO_Median ~ Site + Year +
                        T_Median +
                        s(FlowIndex, k = 5) +
                        Month, 
                       random = list(Year_f = ~ 1),
                       correlation = corAR1(form = ~ as.numeric(sdate) | Site),
                       na.action = na.omit, 
                       method = 'REML',
                       data = full_data))
do_gamm_trend_2$gam$call <- the_call

my_ref_grid <- ref_grid(do_gamm_trend_2, cov.reduce = median, 
                        at = list(Year = 2014)) 
(by_site <- summary(emmeans(my_ref_grid, ~ Site  )))
```

```{r plot_trend_gamm_2_quick}
plot(by_site) +
  xlab('Predicted DO (mg/l)') +
  coord_flip() 
```

Note that we STILL predict low DO for S05 in 2014, but the prediction is
actually not far of the observed averages.
