---
title: "Analysis of LCWMD 'Diurnal Exceedences' of Class C DO Standards"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "01/12/2021"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 3
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
The Long Creek Watershed, almost three and a half square miles in area, is
dominated by commercial land use. The Maine Mall is one of the largest land
owners in the watershed, and it is surrounded by a range of commercial
businesses, from medical offices, to car washes.  About a third of the watershed
in impervious surfaces like roads, parking lots, and rooftops.

Landowners with an acre or more of impervious area are required to get a Clean
Water Act permit for stormwater discharges from their property.  The LCWMD
provides an alternative for landowners to working to receive an individual
permit. Landowners who elect to participate in the The Long Creek Watershed
Management District receive a General Permit, in return for providing funding to
the District, and facilitating the work of the district by permitting access to
their property for certain activities.

For more information on LCWMD, see [their web site](restorelongcreek.org).

Over the past decade, LCWMD has contracted with several consulting firms to
provide  water quality monitoring services along Long Creek.  This has produced
one of the most extensive and best documented data set from the Northeastern US 
looking at water quality conditions in an urban stream.

GZA Geoenvironmental Incorporated (GZA) has been the primary monitoring
contractor for LCWMD for several years, and in 2019, they conducted a thorough
review of LCWMD data. These analyses are based on their summary data sets, and
recapitulate and extend their analyses.

## Are Water Quality Criteria Met?
The primary question we ask in this Notebook, is whether water quality criteria 
pertaining to levels of dissolved oxygen are met. In particular, we explore
various ways of modeling those probabilities, and settle on modeling only 
summertime probabilities as the most informative for State of Casco Bay readers.

We ask whether the probability of failing to meet criteria each day is
changing.  Secondarily, we examine differences among sites in the probability of
failing criteria.

We explored numerous alternate model forms before settling on the ones presented
here.  (See the 'DO_Alternate_Models.Rmd' notebook for details).

The primary challenge was that failure to meet dissolved oxygen 
standards is relatively rare, so many models create cells in the design
matrix in which the probability of meeting water quality standards is one.

This poses significant problems for estimating model parameters.  The models 
presented here are relatively sparse, to avoid those problems.

Critically, we focus ONLY on the July and August, when probability of violating
water quality conditions for dissolved oxygen were highest.  This allowed us to
fit models looking across all years and sites.

## Note
In this data set a "TRUE" value consistently implies that water quality criteria
were met or exceeded, whether that is achieved by a value higher than or lower
than some numeric criteria.  "TRUE" implies good conditions.  "FALSE" implies 
bad conditions.
    
## Sources of Threshold Values  
### Dissolved oxygen
Maineâ€™s Class B water quality standards call for dissolved oxygen above 7 mg/l,
with percent saturation above 75%. The Class C Standards, which apply to almost
all of Long Creek, call for dissolved oxygen above 5 mg/l, with percent
saturation above 60%. In addition, for class C conditions, the thirty day
average dissolved oxygen must stay above 6.5 mg/l.

# Import Libraries  
```{r libraries}
library(nlme)      # Supports glmmPQL()
#library(MASS)      # for glmmPQL() function, which allows correlation in GLM

#library(glmmTMB)   # An alternate -- possibly more robust -- fitting algorithm

library(mgcv)     # For mixed effects GAMM models -- probably not needed here yet.

library(tidyverse)  # Has to load after MASS, so `select()` is not masked
library(readr)

library(emmeans)  # Provides tools for calculating marginal means

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())

library(LCensMeans)
```

# Data Preparation
## Folder References
```{r folders}
sibfldnm    <- 'Derived_Data'
parent      <- dirname(getwd())
sibling     <- file.path(parent,sibfldnm)

dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
dir.create(file.path(getwd(), 'models'), showWarnings = FALSE)
```

## Data on Sites and Impervious Cover
These data were derived from Table 2 from a GZA report to the Long Creek
Watershed Management District, titled "Re: Long Creek Watershed Data Analysis;
Task 2: Preparation of Explanatory and Other Variables."  The Memo is dated
November 13, 2019 File No. 09.0025977.02.

Cumulative Area and IC calculations are our own, based on the GZA data and the
geometry of the stream channel.

```{r IC_data}
# Read in data and drop the East Branch, where we have no data
fn <- "Site_IC_Data.csv"
fpath <- file.path(sibling, fn)

Site_IC_Data <- read_csv(fpath) %>%
  filter(Site != "--") 

# Now, create a factor that preserves the order of rows (roughly upstream to downstream). 
Site_IC_Data <- Site_IC_Data %>%
  mutate(Site = factor(Site, levels = Site_IC_Data$Site))

# Finally, convert percent covers to numeric values
Site_IC_Data <- Site_IC_Data %>%
  mutate(CumPctIC = as.numeric(substr(CumPctIC, 1, nchar(CumPctIC)-1))) %>%
  mutate(PctIC = as.numeric(substr(PctIC, 1, nchar(PctIC)-1)))
Site_IC_Data
```

## Main Data
We remove 2019 data, as we don't have a complete year's worth of data, which may
bias annual summaries.

Note that this data does NOT include all of the predictors used in some models
looking at chlorides. In particular, it does not include stream flow estimates 
```{r main_data}
fn <- "Exceeds_Data.csv"
exceeds = read_csv(file.path(sibling, fn), progress=FALSE) %>%
  mutate(IC=Site_IC_Data$CumPctIC[match(Site, Site_IC_Data$Site)]) %>%
  select(-X1) %>%
  filter(Year < 2019) %>%
  mutate(Site = factor(Site, levels=levels(Site_IC_Data$Site)),
         year_f = factor(Year),
         month_f = factor(Month, levels = 1:12, labels = month.abb),
         DOY = as.numeric(format(sdate, format = '%j')),
         season = cut(Month, breaks = c(0,2,5,8,11,13),
                      labels = c('Winter', 'Spring',
                                 'Summer', 'Fall', 'Winter')),
         season = factor(season, levels = c('Winter', 'Spring', 
                                           'Summer', 'Fall'))) %>%
  mutate(lPrecip = log1p(Precip))
```

## Data Corrections
### Anomolous Depth Values
Several depth observations in the record appear highly unlikely. In particular,
several observations show daily median water depths over 15 meters. A few other
observations show daily median depths over 4 meters, which also looks unlikely
in a stream of this size.  All these events also occurred in May or June of 2015
at site S05. Some sort of malfunction of the pressure transducer appears likely.

We can trace these observations back to the raw QA/QC'd pressure and sonde data 
submitted to LCWMD by GZA, so they are not an artifact of our data preparation.

We remove these extreme values.  The other daily medians in May and June of 2015
appear reasonable, and we leave them in place, although given possible 
instability of the pressure sensors, it might make sense to remove them all.

Note that removing depth observations from Site S05  will remove those DATES
from any model that uses the `FlowIndex` variable (see below) as a predictor.
```{r correct_depth_data}
exceeds <- exceeds %>%
  mutate(D_Median = if_else(D_Median > 4, NA_real_, D_Median),
         lD_Median = log1p(D_Median))
```

### Single S06B Chloride Observation from 2017
The data includes just a single chloride observation from site S06B from
any year other than 2013.  While we do not know if the data point is legitimate
or not, it has  high leverage in several models, and we suspect a 
transcription error of some sort.

We remove the Chloride value from the data.
```{r correct_chloride_data}
exceeds <- exceeds %>%
  mutate(ChlCCC = if_else(Site == 'S06B' & Year > 2014,
                              NA, ChlCCC),
         ChlCMC = if_else(Site == 'S06B' & Year > 2014,
                              NA, ChlCMC))
```


### Anomolous Dissolved Oxygen and Chloride Values
#### Site S03, end of 2016
We noted  extreme dissolved oxygen data at the end of 2016.  Values were
both extreme and highly variable.  (See discussion in the DO Analysis workbooks).

We decide we should remove chloride and oxygen observations after October 15th.

```{r correct_S03_October_2016_data}
exceeds <- exceeds %>% 
  mutate(ChlCCC = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ChlCCC),
         ChlCMC = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ChlCMC),
         ClassCDO = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ClassCDO),
         ClassBDO = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ClassBDO),
         ClassC_PctSat = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ClassC_PctSat),
         ClassB_PctSat = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ClassB_PctSat),
         ClassCBoth = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ClassCBoth),
         ClassBBoth = if_else(Year == 2016 & Site == 'S03' & DOY > 288,
                              NA, ClassBBoth))
```

## Remove Partial Data from Winter Months
We have very limited data from several months.  We have January data 
from only one year, and February data from only two, and December data from only
four years, all older.  Both March and November sample sizes vary.

The limited winter data generates severely unbalanced samples, which may lead to estimation problems, especially in models with crossed or potentially crossed
factors and predictors.  More fundamentally, the potential bias introduced by
showing data from those months from just a handful of years could give a
misleading impression of seasonal patterns.  We trim December, January and
February data, but leave the other months. 

It is important to remember,  even after trimming the data, that:  
1.  2010 is a partial year,  
2.  The period of sampling in March may be biased due to spring melt timing.

```{r month_by_year_crosstab}
xtabs(~ year_f + month_f, data = exceeds)
```

```{r remove_limited_winter_data}
exceeds <- exceeds %>%
  filter(Month >= 3 & Month <= 11)
```

## Add Stream Flow Index
We worked through many models on a site by site basis in which we included data
on water depth, but since the depth coordinate is site-specific, a 10 cm depth
at one site may be exceptional, while at another it is commonplace. We generally
want not a local measure of stream depth, but a watershed-wide metric of high,
medium, or low stream flow.

Middle and Lower Main Stem sites would be suitable for a general flow indicator
across the watershed. The monitoring sites in that stretch of Long Creek include
include S05 and S17, however only site S05 has been in continuous operation
throughout the period of record, so we use depth data from S05 to construct
our general stream flow indicator.

Stream flow at S05 is correlated with flow at other sites, although not all that
closely correlated to flow in the downstream tributaries (S01 and S03).
```{r depth_correlations}
exceeds %>%
  select(sdate, Site, lD_Median) %>%
  pivot_wider(names_from = Site, values_from = lD_Median) %>%
  select( -sdate) %>%
  cor(use = 'pairwise', method = 'pearson')

```
We use the log of the daily median flow at S05 as a general watershed-wide
stream flow indicator, which we call `FlowIndex`.  We use the log of the raw
median, to lessen the effect of the highly skewed distribution of stream depths
on the metric. The resulting index is still highly skewed.

```{r FlowIndex}
depth_data <- exceeds %>%
  filter (Site == 'S05') %>%
  select(sdate, lD_Median)

exceeds <- exceeds %>%
  mutate(FlowIndex = depth_data$lD_Median[match(sdate, depth_data$sdate)])

rm(depth_data)
```

## Create July and August Data Only
```{r filter_July_August}
exceeds_two <- exceeds %>%
  filter(Month  > 6 & Month < 9)
```

# Initial Cross Tabs
## Utility Function
This function just adds a percent summary column to a cross-tab.
```{r utility_fxn}
xt_pct <- function(.form, .dat) {
  xt <- xtabs(.form, data = .dat)
  xt <- cbind(xt, round(apply(xt, 1, function(X) X[1]/sum(X)), 3)*100)
  names(xt[3]) <- 'Percent Fail'
  return(xt)
}
```

## Dissolved Oxygen
```{r do_xtab}
xt_pct(~Year + ClassCDO, exceeds)
```

## Percent Saturation
```{r ps_xtab}
xt_pct(~Year + ClassC_PctSat, exceeds)
```

The two oxygen-related exceedences are correlated.  IN particular, no samples
met the Percent Saturation standard, but failed the dissolved oxygen standard.
```{r both_xtab}
xtabs(~ ClassCDO + ClassC_PctSat, data = exceeds)
```

# Dissolved Oxygen
## Exploratory Graphics
These are estimated as empirical relative frequencies, with error estimated 
as two times the standard error of the estimate. 
```{r do_site_empirical_p}
exceeds %>%
  group_by(Site, Year) %>%
  summarize(do_true = sum(ClassCDO, na.rm = TRUE),
            do_count = sum(! is.na(ClassCDO)),
            do_p = do_true/do_count,
            do_err = do_p*(1-do_p)/sqrt(do_count),
            .groups = 'drop') %>%
  ggplot(aes(Year, do_p, color = Site)) +
  geom_line() +
  geom_pointrange(aes(ymin = do_p-2 * do_err, ymax = do_p + 2 * do_err)) +
  ylab('Probability of Passing DO Standard')
```
2016 was a rough year at most sites.

Note that for some year/ site combination, we never had a failure to meet DO 
standards.  This limits models we can fit.

```{r do_month_empirical_p}
exceeds  %>%
  group_by(month_f, Year) %>%
  summarize(do_true = sum(ClassCDO, na.rm = TRUE),
            do_count = sum(! is.na(ClassCDO)),
            do_p = do_true/do_count,
            do_err = do_p*(1-do_p)/sqrt(do_count)) %>%
  ggplot(aes(Year, do_p, color = month_f)) +
  geom_line() +
  geom_pointrange(aes(ymin = do_p-2 * do_err, ymax = do_p + 2 * do_err))
```
 That shows that 2016 was a tough year in June, July, August, and September,
 while June and July were tough in 2018.  This highlights the role of drought 
 in shaping conditions in Long Creek regarding dissolved oxygen.

## GAMM with Autocorrelated Error
Multiple attempts at a GAMM models  had convergence problems, but provided 
insight into model formulation. These models sometimes took over an hour to run, 
without reaching convergence. Other models ran into other kinds of estimability
problems for some key predictors, so we ended up with a relatively simple
model:

A model that omits the flow index term converges rapidly (~ 5 min, 7 iterations). 
```{r do_gamm_2, cache = TRUE}
if (! file.exists("models/do_gamm_two_2.rds")) {
  system.time(
    do_gamm_two_2<- gamm(ClassCDO ~ Site + year_f + MaxT + 
                         s(lPrecip),
                       correlation = corCAR1(form = ~ sdate | Site),
                       family = 'binomial',
                       niterPQL = 50, verbosePQL = TRUE,
                       data = exceeds_two)
)
  saveRDS(do_gamm_two_2, file="models/do_gamm_two_2.rds")
} else {
  do_gamm_two_2 <- readRDS("models/do_gamm_two_2.rds")
}
```

```{r}
summary(do_gamm_two_2$gam)
```

`gam.check()` is only marginally useful for a binomial GAM, but we look anyway.
```{r}
gam.check(do_gamm_two_2$gam)
```

### Extract and Plot Marginal Means
```{r do_construct_call}
the_call <-  quote(gamm(ClassCDO ~ Site + year_f + MaxT + 
                         s(lPrecip),
                       correlation = corCAR1(form = ~ sdate | Site),
                       family = 'binomial',
                       niterPQL = 50, verbosePQL = TRUE,
                       data = exceeds_two))
do_gamm_two_2$gam$call <- the_call
```

#### By Site
```{r do_m2_mm_by_site}
my_ref_grid <- ref_grid(do_gamm_two_2,  cov.reduce = median) 
(b <- emmeans(my_ref_grid, ~ Site, type = 'response'))
```

##### Graphics
```{r do_m2_mm_graphic_by_site, fig.width = 4, fig.height = 3}
s <- summary(b) %>% 
  mutate(fprob = 1-prob,
         fUCL = 1 - lower.CL,
         fLCL = 1 - upper.CL)

ggplot(s, aes(Site, fprob)) +
 
  geom_pointrange(aes(ymin = fLCL, ymax = fUCL),
                color = cbep_colors()[1]) +
  
  ylab('Probability of Failing\nClass C DO Standard') +
  xlab('     Upstream        Main Stem       Downstream') +
  ggtitle('July and August Only') +
  theme_cbep(base_size = 12) +
  theme(axis.title.x = element_text(size = 10))
```

```{r do_m2_mm_bars_by_site,  fig.width = 4, fig.height = 3}
ggplot(s, aes(Site, fprob)) +
  geom_col(fill = cbep_colors()[4]) + 
  geom_linerange(aes(ymin = fLCL, ymax = fUCL),
                color = cbep_colors()[1]) +
  
  ylab('Probability of Failing\nClass C DO Standard') +
  xlab('Upstream                    Main Stem                 Downstream') +
  ggtitle('July and August Only') +
  theme_cbep(base_size = 12) +
  theme(axis.title.x = element_text(size = 10))
```

#### By Year
```{r do_mm_by_years}
my_ref_grid <- ref_grid(do_gamm_two_2,  cov.reduce = median) 
(b <- emmeans(my_ref_grid, ~ year_f, type = 'response'))

```

##### Graphics
```{r do_mm_graphic_by_years, fig.width = 4, fig.height = 3}
s <- summary(b)

s %>% 
  mutate(fprob = 1-prob,
         fUCL = 1 - lower.CL,
         fLCL = 1 - upper.CL) %>%
ggplot(aes(as.numeric(year_f) + 2009, fprob)) +
 
  geom_pointrange(aes(ymin = fLCL, ymax = fUCL),
                color = cbep_colors()[1]) +
  geom_line(color = cbep_colors()[3]) +
  
  ylab('Probability of Failing\nClass C DO Standard') +
  xlab('') +
  ggtitle('July and August Only') +
  theme_cbep(base_size = 12)
```

# Percent Saturation
## Exploratory Graphics
These are estimated as empirical relative frequencies, with error estimated 
as two times the standard error of the estimate. 
```{r ps_site_empirical_p}
exceeds %>%
  group_by(Site, Year) %>%
  summarize(psat_true = sum(ClassC_PctSat, na.rm = TRUE),
            psat_count = sum(! is.na(ClassC_PctSat)),
            psat_p = psat_true/psat_count,
            psat_err = psat_p*(1-psat_p)/sqrt(psat_count),
            .groups = 'drop') %>%
  ggplot(aes(Year, psat_p, color = Site)) +
  geom_line() +
  geom_pointrange(aes(ymin = psat_p-2 *psat_err, ymax = psat_p + 2 * psat_err)) +
  ylab('Probability of Passing\nPercent Saturation Standard')
```

Note that for some year/ site combination, we never had a failure to meet 
standards.  This limits models we can fit, as it did for DO.
```{r ps_month_empirical_p}
exceeds  %>%
  group_by(month_f, Year) %>%
  summarize(psat_true = sum(ClassC_PctSat, na.rm = TRUE),
            psat_count = sum(! is.na(ClassC_PctSat)),
            psat_p = psat_true/psat_count,
            psat_err = psat_p*(1-psat_p)/sqrt(psat_count)) %>%
  ggplot(aes(Year, psat_p, color = month_f)) +
  geom_line() +
  geom_pointrange(aes(ymin = psat_p-2 * psat_err, ymax = psat_p + 2 * psat_err))
```

## GAMM with Autocorrelated Error
We follow the format of the DO model just developed. This model takes ~ 15 
minutes to fit and converges in 6 iterations.
```{r ps_gamm_2, cache = TRUE}
if (! file.exists("models/psat_gamm_two_2.rds")) {
  system.time(
    psat_gamm_two_2<- gamm(ClassC_PctSat ~ Site + year_f + MaxT + 
                         s(lPrecip),
                       correlation = corCAR1(form = ~ sdate | Site),
                       family = 'binomial',
                       niterPQL = 50, verbosePQL = TRUE,
                       data = exceeds_two)
)
  saveRDS(psat_gamm_two_2, file="models/psat_gamm_two_2.rds")
} else {
  psat_gamm_two_2 <- readRDS("models/psat_gamm_two_2.rds")
}
```

### Extract and Plot Marginal Means
```{r ps_construct_call}
the_call <-  quote(gamm(ClassC_PctSat ~ Site + year_f + MaxT + 
                         s(lPrecip),
                       correlation = corCAR1(form = ~ sdate | Site),
                       family = 'binomial',
                       niterPQL = 50, verbosePQL = TRUE,
                       data = exceeds_two))
psat_gamm_two_2$gam$call <- the_call
```

#### By Site
```{r ps_mm_by_Site}
my_ref_grid <- ref_grid(psat_gamm_two_2,  cov.reduce = median) 
(b <- emmeans(my_ref_grid, ~ Site, type = 'response'))
```

##### Graphics
```{r ps_mm_graphic_by_site, fig.width = 4, fig.height = 3}
s <- summary(b) %>% 
  mutate(fprob = 1-prob,
         fUCL = 1 - lower.CL,
         fLCL = 1 - upper.CL)

ggplot(s, aes(Site, fprob)) +
 
  geom_pointrange(aes(ymin = fLCL, ymax = fUCL),
                color = cbep_colors()[1]) +
  
  ylab('Probability of Failing\nClass C\nPercent Saturation Standard') +
  xlab('     Upstream        Main Stem       Downstream') +
  ggtitle('July And August Only') +
  theme_cbep(base_size = 12) +
  theme(axis.title.x = element_text(size = 10))
```

#### By Year
```{r ps_mm_by_year,}
my_ref_grid <- ref_grid(psat_gamm_two_2,  cov.reduce = median) 
(b <- emmeans(my_ref_grid, ~ year_f, type = 'response'))
```

##### Graphics
```{r ps_mm_graphic_by_year,}
ggplot(s, aes(Site, fprob)) +
  geom_col(fill = cbep_colors()[2]) + 
  geom_linerange(aes(ymin = fLCL, ymax = fUCL),
                color = cbep_colors()[1]) +
  
 ylab('Probability of Failing\nClass C\nPercent Saturation Standard') +
  xlab('Upstream                  Main Stem                Downstream ') +
  ggtitle('July And August Only') +
  theme_cbep(base_size = 12) +
  theme(axis.title.x = element_text(size = 10))
```

